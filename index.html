<!DOCTYPE html>
<head>
	<!-- got this config from here: http://haixing-hu.github.io/programming/2013/09/20/how-to-use-mathjax-in-jekyll-generated-github-pages/ -->
	<!-- some changes due to different mathjax version -->
	<script type="text/x-mathjax-config">
  		MathJax.Hub.Config({
    			tex2jax: {
      				inlineMath: [ ['$','$'], ['($', '$)' ]],
      				displayMath: [ ['$$','$$'] ],
      				processEscapes: true,
   			 }
  		});
	</script>
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
	<style>
		.column {
			float: left;
		}

		.left {
			width: 30%;
			height: 100%;
			overflow: scroll;
		}

		.left a:link, .left a:visited {
			color: #586069!important;
		}

		.left a:hover, .left a:active {
			color: #ba2121!important;
		}

		.left a {
			text-decoration: none;
		}

		.right {
			width: 70%;
			height: 100%;
			overflow: scroll;
		}

		html, body {
			color: #586069!important;
			font-family: Arial, Helvetica, sans-serif;
			height: 98%;
		}

		.row {
			height: 98%;
		}

		code { /* style copied from github, text color slightly modified */
		    padding: .2em .4em;
			margin: 0;
			font-size: 85%;
			background-color: rgba(27,31,35,.05);
			border-radius: 3px;
			color: #ba2121;
		}
		
	</style>
</head>
<body>
<div class="row">
<div class="column left">
	<h2><a href="#top">CartoonGAN</a></h2>
	<ul>
	<li><a href="#tc1">Generate dataset</a></li>
		<ul>
			<li><a href="#tc1_1">Cartoon images</a></li>
			<li><a href="#tc1_2">Edge-smoothed version of cartoon images</a></li>
			<li><a href="#tc1_3">Photos</a></li>
		</ul>
	<li><a href="#tc2">Prepare to train within Google Colab</a></li>
		<ul>
			<li><a href="#tc2_1">Transfer data via Google Drive</a></li>
		</ul>
	<li><a href="#tc3">Data loader</a></li>
	<li><a href="#tc4">Define model</a></li>
		<ul>
			<li><a href="#tc4_1">Padding</a></li>
			<li><a href="#tc4_2">Stride</a></li>
			<li><a href="#tc4_3">Learnings</a></li>
		</ul>
	<li><a href="#tc5">Loss function</a></li>
		<ul>
			<li><a href="#tc5_1">Adversarial loss</a></li>
			<li><a href="#tc5_2">Content loss</a></li>
			<li><a href="#tc5_3">Learnings</a></li>
			<ul>
				<li><a href="#tc5_4">Loss function implementation</a></li>
				<li><a href="#tc5_5">Parameter $\omega$</a></li>
				<ul>
					<li><a href="#tc5_6">Training round 1</a></li>
					<li><a href="#tc5_7">Training round 2</a></li>
					<li><a href="#tc5_8">Training round 3</a></li>
					<li><a href="#tc5_9">Training round 4</a></li>
				</ul>
			</ul>
		</ul>
	<li><a href="#tc6">Optimizer</a></li>
	<li><a href="#tc7">Transforming an image from local filesystem</a></li>
	<li><a href="#tc8">Credits</a></li>
	<li><a href="#notes">Notes/next steps</a></li>
	<li><a href="#refs">References</a></li>
	<p>
		<a href="https://github.com/TobiasSunderdiek/cartoon-gan">View project on GitHub</a>
	</p>
</div>

<div class="column right">
	<h1><a name="top"/>CartoonGAN - my attempt to implement it</h1>

Within <a href="https://github.com/TobiasSunderdiek/cartoon-gan">this repo</a>, I try to implement a cartoon GAN as described in <a href="#ref_1">this paper [1]</a> with PyTorch.

<p><img src="assets/result.png" alt="result example"/></p>

<h2><a name="tc1"/>Generate dataset</h2>
For training the GAN, photos and cartoon images are needed. I provide scripts to locally download these images from public and legally-to-use sources. As this are not the same input data as used in the paper, this may be a point of discussion. See the following sections for more information about the data I used.
<h3><a name="tc1_1"/>Cartoon images</h3>

In the original paper, the data was produced by extracting images from anime movies, e.g. the movie "Spirited Away" by Miyazaki Hayao.<br />

Due to copyright reasons I tried to find an alternative dataset, which I have found in the safebooru <a href="#ref_2">[2]</a> dataset from kaggle. Problem of this procedure are the different creators and their different styles of the images. This will lead to training data from which it is hard to learn from. This will not train the network properly I think, but I give it a try. In the paper, only one style from one artist is used.<br />
<br />
I get the cartoon part of the dataset for my implementation of the cartoon GAN the following way:
<ul>
	<li>download <code>all_data.csv</code> from safebooru dataset <a href="https://www.kaggle.com/alamson/safebooru/download">here</a></li>
	<li>point to <code>all_data.csv</code> in <code>PATH_TO_SAFEBOORU_ALL_DATA_CSV</code> of <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/cartoon_image_downloader.py"><code>cartoon_image_downloader.py</code></a></li>
	<li>configure the folder where you want the script to download the images to in <code>PATH_TO_STORE_DOWNLOADED_CARTOON_IMAGES</code></li>
	<li>the script creates a .zip-file of the downloaded images, configure path where to store resulting .zip-file in <code>CARTOON_IMAGES_ZIPFILE_NAME</code></li>
	<li>run <code>make install</code> to install necessary libraries</li>
	<li>run <code>make cartoons</code>to download configurable amount of medium size images</li>
</ul>
This step is resumeable.
<h3><a name="tc1_2"/>Edge-smoothed version of cartoon images</h3>

To make the GAN better learn to produce clear edges in the cartoon image, the model is trained with an edge-smoothed version on every cartoon image, too. In the paper, the edges are first detected by canny-edge, then the edges are dilated and smoothed with gaussian smoothing. In my implementation, I do the canny edges, the dilation and the gaussian blur with openCV and I make the white backbground transparent and paste the edges back on the original image with Pillow.<br />
<br />
I create a edge-smoothed version of every cartoon image:<br />
<ul>
	<li>use downloaded images from safebooru as described in <a href="#tc1_1">previous step</a></li>
	<li>configure where the cartoon images are stored in <code>PATH_TO_STORED_CARTOON_IMAGES</code> in <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/cartoon_image_smoothing.py"><code>cartoon_image_smoothing.py</code></a></li>
	<li>configure where to store smoothed images in <code>PATH_TO_STORE_SMOOTHED_IMAGES</code></li>
	<li>configure where to store resulting .zip-file of images in <code>SMOOTHED_IMAGES_ZIPFILE_NAME</code></li>
	<li>run <code>make cartoons-smooth</code> to create the images</li>
</ul>
This step is resumeable.
<h3><a name="tc1_3"/>Photos</h3>
In the paper, photos are downloaded from flickr. In my implementation I try to use the COCO <a href="#ref_3">[3]</a> dataset, especially the category <b>person</b>.<br />

I get the photos for the dataset by following the COCO part as described in <a href="#ref_4">[4]</a>:<br />
<ul>
	<li>download and unzip coco annotations from <a href="#ref_5">[5]</a></li>
	<li>configure annotations dir location in <code>PATH_TO_COCO_ANNOTATIONS_ROOT_FOLDER</code> of <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/photo_downloader.py"><code>photo_downloader.py</code></a></li>
	<li>configure where to store photos in <code>PATH_TO_STORE_DOWNLOADED_PHOTOS</code></li>
	<li>configure where to store resulting .zip-file of images in <code>PHOTO_ZIPFILE_NAME</code></li>
	<li>run <code>make photos</code> to download configurable amount of photos of persons</li>
</ul>
This step is resumeable.
<h2><a name="tc2"/>Prepare to train within Google Colab</h2>

As Google Colab offers free GPU-usage in a jupyter notebook, I used this service and declared all the necessary elements e.g. the model or optimizer within a notebook. Transfering the training data and saving some results is done via Google Drive. Due to the whole training took about 24h, I made the processes in the notebook resumeable.

<p><a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb">My Google Colab Notebook is located here, can be opened in Colab via the button on top of the displayed file if wanted.</a></p>

<h2><a name="tc2_1"/>Transfer data via Google Drive</h2>
As a first step, I transfer the in the <a href="#tc1">previous step</a> generated images from my local machine to Google Drive and connect the jupyter notebook in Google Colab to this drive. After that I directly copied the files to the notebook folder. I described the details of this step within the <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb">colab notebook</a>.

<h2><a name="tc3"/>Data loader</a></h2>
I create a data loader for every kind of input image (cartoons/smoothed cartoons/photos) to transform the images and to split into training and validation sets by a 90/10-ratio.<br />
<br />
As mentioned in the paper, the used image size is 256x256 pixel. The Generator uses ReLu as activation function, which generates values in the range of [0.0, 1.0]. As the <code>ToTensor()</code>-method changes the range of the input image from RGB [0, 255] to [0.0, 1.0], we get the same range for all images.
<p><code>
	image_size = 256<br />
	batch_size = 16<br />
	<br />
	transformer = transforms.Compose([<br />
	transforms.CenterCrop(image_size),<br />
	transforms.ToTensor()])<br />
	<br />
	cartoon_dataset = ImageFolder('cartoons/', transformer)<br />
	len_training_set = math.floor(len(cartoon_dataset) * 0.9)<br />
	len_valid_set = len(cartoon_dataset) - len_training_set<br />
	<br />
	training_set, _ = random_split(cartoon_dataset, (len_training_set, len_valid_set))<br />
	cartoon_image_dataloader_train = DataLoader(training_set, batch_size, shuffle=True, num_workers=0)<br />
</code></p>
Example for cartoon images data-loader, data-loaders for smoothed version and photos are configured the same, see <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb">Colab notebook</a> for more details.

<h2><a name="tc4">Define model</a></h2>
The information about the model structure is given in the paper. See <a href="https://github.com/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb">Colab notebook</a> for model definition. <br />
<br />
<h2><a name="tc4_1">Padding</h2>
For the <code>zero padding</code> of the convolutional layers, I use the following formula:<br />
<p>
$$Height x Width_{output} = \frac{HeightxWidth_{input} - kernel size + 2 padding}{stride} + 1$$

e.g:<br />
<br />
<code>conv_1</code> layer of generator: $HxW$ should stay the same as input size, which is 256x256 and <code>stride = 1</code><br />

$$256 = \frac{256-7+2padding}{1}+1, padding = 3$$

In case of a fraction as a result, I choose to ceil:<br />
<br />
<code>conv_2</code> layer of generator: $\frac{H}{2} x \frac{W}{2}$ is output with <code>stride=2</code>

$$128 = \frac{256-3+2padding}{2}+1, padding= \frac{1}{2} \Rightarrow padding=1$$
</p>

<h2><a name="tc4_2">Stride</a></h2>
In the up-convolutional part of the paper, two layers (conv_6 and conv_8 in my implementation) have a stride of 0.5. As PyTorchs <code>conv2D()</code> does not allow floating point stride, I use a stride of 1 in both cases. Therefore even for the padding calculation I go with a stride of 1.

<h2><a name="tc4_3">Learnings</a></h2>

After implementing the generator and getting some results out of the training, the generated images have a wide grey margin, like in this example image:<br />

<p><img src="assets/grey_margin.jpg" alt="failure image" /></p>

<p>I re-checked my implementation of the generator and stumbled across my interpretation of the stride for <code<conv_6</code> and <code>conv_8</code>, which is $\frac{1}{2}$ in the paper. Maybe I got the part of the stride wrong, and this is not $\frac{1}{2}$, but a tuple of $(1,2)$? If so, I did a wrong padding calculation. This are the only layers where I calculated a very large padding of 33 and 65, which looks suspicious now.
Testing a tuple of $(1,2)$, I also ended up with very high values for the padding.</p>
<p>The next problem was, that I used <code>Conv2d</code> for up-sampling, but <code>Conv2d</code> is for down-sampling. <code>ConvTranspose2d</code> is for up-sampling, see <a href="#ref_6">[6]</a> and <a href="#ref_7">[7]</a>. I corrected my implementation accordingly.</p>

<p>By using <code>ConvTranspose2d</code> with the values for <code>stride</code> (1 or (1,2)) and <code>kernelsize</code>, and playing with <code>padding</code>, the resulting image keeps nearly the same dimension, shrinks or gets uneven dimensions. 

$$ HeightxWidth_{Output} = stride (HeightxWidth_{Input} - 1) + kernelsize - 2*padding$$

$$ HeightxWidth_{Output} = 1 * (64 - 1) + 3 - 2 * padding = 66 - 2 * padding = \bigg\{^{66, p = 0}_{<0, p < 0}$$
<br />
Uneven dimensions: I tested stride $(1,2)$ with padding $(3,2)$ and got $60x125$ as image size.

<p>But as mentioned in the paper, I need to scale from $\frac{H}{4}$ up to $\frac{H}{2}$, which is from 64 to 128, and then up to 256 in <code>conv_8</code> and <code>conv_9</code>. Therefore I decided to use <code>stride=2</code> and <code>padding=1</code> in <code>conv_6</code>, and <code>stride=2</code> and <code>padding=1</code> in <code>conv_8</code>. To add the last pixel, I add an <code>output_padding</code> of 1.</p>

<p><code>conv_6</code>: $2*(64-1)+3-(2*1)=127 + 1$ (outer_padding)</p>

<p><code>conv_8</code>: $2*(128-1)+3-(2*1)=255 + 1$ (outer_padding)</p>

My implementation differs from the paper in the mentioned layers.

<h2><a name="tc5">Loss function</a></h2>
$$\mathcal{L}(G, D) = \mathcal{L}_{adv}(G, D) + ω\mathcal{L}_{con}(G, D), \omega=10$$

This loss is used to train the discriminator and the generator. In the adversarial part, the discriminator tries to classify the generated images as fakes.

During the generator training, the generator tries to minimize the classifications, where the discriminator classifies the generated image as fake. The generator has only affect on the parts of the formula where $G()$ is involved, so the generator tries to minimize this part. Additionally, the loss is not directly calculated from the generator output, but from the discriminator output. Due to the fact that the generator output is the input for the discriminator output in the generator training, the generator is in the chain of the backpropagation, when the loss from the discriminator output is backprogagated all the way back through the discriminator model and generator model to the photo image input data, see <a href="#ref_8">[8]</a> and <a href="#ref_9">[9]</a>.

<h2><a name="tc5_1">Adversarial loss</a></h2>
The adversarial loss  $\mathcal{L}_{adv}(G, D)$ which drives the generator to transform photo to comic style of the image. Its value indicates if the output looks like a cartoon image or not. The paper highlights, that a characteristic part of cartoons images are the clear edges, which are a small detail of the image, must be preserved to generate clear edges in the result. In the paper, this is solved by training not only with cartoon images but additionaly by training with the same cartoon images with smoothed edges so that the discriminator can distinguish between clear and smooth edges. For achieving this the authors define the edge-promoting adversarial loss function:

$$\mathcal{L}_{adv}(G, D) = \mathbb{E}_{ci∼S_{data}(c)}[log D(c_i)] + \mathbb{E}_{ej∼S_{data}(e)}[log(1 − D(e_j))] + \mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$$

<p>For the discriminator, this is the formula for the loss function, because output of the Discriminator plays no role within the content loss part of the loss function.</p>
<p>For the initialization phase of the generator, this part of the formula is not used as described in the paper.</p>
<p>For the training phase of the generator, only the part of the formula is used within the generator loss function, which the generator can affect: $$\mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$$</p>

<h2><a name="tc5_2">Content loss</a></h2>
The content loss $ω\mathcal{L}_{con}(G, D)$, which preserves the semantic content during transformation. To calculate this, in the paper the high-level feature maps of the VGG network is used, in particular the layer $l=$ <code>conv4_4</code>. The output of the layer $l$ for the original photo is subtracted from the output of the layer $l$ of the generated image. The result is regularized using the $\mathcal{L_1}$ spare regularization ($||...||_1$):

$$\mathcal{L}_{con}(G, D)= \mathbb{E}_{pi~S_{data}(p)}[||VGG_l(G(p_i))-VGG_l(p_i)||_1]$$

This part of the formula plays a role in the loss function for the generator, not for the discriminator, because only the generator is used within this formula.

More info about $\mathcal{L_1}$ regularization in <a href="#ref_10">[10]</a> and <a href="#ref_11">[11]</a>.

<h2><a name="tc5_3">Learnings</a></h2>
<p>At this section I will describe my learnings during implementing/testing the loss functions.</p>
<h2><a name="tc5_4">Loss function implementation</a></h2>
At my first implementation, I took the output of the discriminator with size <code>batch_size x 1 x 64 x 64</code> as input to the discriminator loss (to be precise, all three outputs of <code>D</code>, from <code>D(cartoon_image)</code>, <code>D(smoothed_cartoon_image)</code> and <code>D(G(photo))</code>).
As the adversarial loss outputs a probability which indicates if the input is detected as fraud or not, it returns a single value. To reach this, I took the input tensor with shape <code>batch_size x 1 x 64 x 64</code>, and implemented the loss function 

$$\mathcal{L}_{adv}(G, D) = \mathbb{E}_{ci∼S_{data}(c)}[log D(c_i)]
+ \mathbb{E}_{ej∼S_{data}(e)}[log(1 − D(e_j))]
+ \mathbb{E}_{pk∼S_{data}(p)}[log(1 − D(G(p_k)))]$$

manually as 

$$torch.log(torch.abs(D(...)) + torch.log(torch.abs(1 - D(...)) + torch.log(torch.abs(1 - D(...))$$

As the discriminator output sometimes contains negative values, calling <code>log()</code> directly with this value causes an error. Therefore I wrapped <code>abs()</code> around the input of <code>log()</code>.

<p>As my training results weren't as expected, I came back to the loss functions. As an adversarial loss outputs a probability, thus a single value, my discriminator outputs a tensor with shape <code>batch_size x 1 x 64 x 64</code>.</p>

So to get a probability out of this tensor, either an activation function is needed, or I made a mistake in my implementation of the discriminator and it should output a probability. After thinking about this, I went to use an activation function. To reach this, I can either activate the output of the last layer of the discriminator, or I can use a loss function like <code>BCEWithLogitsLoss</code>, which combines activation function and loss.

<p>But which activation function to use?</p>

<p>As the discriminator should give a probability and only has two classes as outputs, <code>real</code> or <code>fake</code>, using sigmoid or softmax is a good choice. Softmax can be used for binary classification as well as classification of $n$-classes.</p>

<p>First, I decided to use a loss function, which combines activation and loss function, and this gave me the choice between:</p>

<ul>
	<li><code>BCEWithLogitsLoss</code>: Sigmoid and binary cross entropy loss</li>
	<li><code>CrossEntroyLoss</code>: Softmax and negative log likelihood loss</li>
</ul>

<p>For solving a minimax-problem, which loss to choose?</p>

<blockquote cite="#ref_12"><p>"If [minimax] implemented directly, this would require changes be made to model weights using stochastic ascent rather than stochastic descent. It is more commonly implemented as a traditional binary classification problem with labels 0 and 1 for generated and real images respectively."</p><footer>see <cite><a href="#ref_12">[12]</a></cite></footer></blockquote>

<p>Therefore I choosed <code>BCEWithLogitsLoss</code>.</p>

<p>As <code>BCEWithLogitsLoss</code> has two parameters, one for the input and one for the target, I used <code>BCEWithLogitsLoss</code> three times, one for every different input, and added the values up.</p>

<p>But, after trying to go with this solution, the generator produces values lower than zero. This lead to problems when trying to map these values to RGB. Therefore I decide to not combine activation and loss function, and use sigmoid in the generator as well as in the discriminator directly and use <code>BCELoss</code> as loss function.</p>

<h2><a name="tc5_5">Parameter $\omega$</a></h2>

<h2><a name="tc5_6">Training round 1</a></h2>
<p>Initially, I set $\omega$, which is a weight to balance the style and the content preservation, to the value given in the paper, which is 10. After running 210 epochs, the content preservation was very good, but the generated images do not have cartoon styles. Maybe this is a problem with my input data, where I use different cartoon styles from different artists instead from one single artist, as used in the paper.</p>

<table>
	<tr>
		<td>direct after start, one of the first epochs</td><td></td>
	</tr>
	<tr>
		<td><img src="assets/no_margin.jpg" alt="no_margin.jpg" width="200"/></td>
		<td></td>
	</tr>
	<tr>
		<td>direct after init-phase is completed</td><td></td>
	</tr>
	<tr>
		<td>Photo input</td><td>Generated image</td>
	</tr>
	<tr>
		<td><img src="assets/epoch_10_photo_input.jpg" alt="epoch_10_photo_input" width="200" /></td>
		<td><img src="assets/epoch_10_generated_image.jpg" alt="epoch_10_generated_image" width="200"/></td>
	</tr>
	<tr>
		<td>direct at the beginning of epoch 11 with use of full generator loss instead of init loss. These results seem to be outliers at this stage of training due to the next outputs look more similar like the inputs</td><td></td>
	</tr>
	<tr>
		<td>Photo input</td><td>Generated image</td>
	</tr>
	<tr>
		<td><img src="assets/epoch_11_photo_input.jpg" alt="epoch_11_photo_input_example_1" width="200"/></td>
		<td><img src="assets/epoch_11_generated_image.jpg" alt="epoch_11_generated_image_example_1" width="200"/></td>
	</tr>
	<tr>
		<td><img src="assets/epoch_11_photo_input_2.jpg" alt="epoch_11_photo_input_example_2"/></td>
		<td><img src="assets/epoch_11_generated_image_2.jpg" alt="epoch_11_generated_image_example_2" width="200"/></td>
	</tr>
	<tr>
		<td>After training has finished 210 epochs, the output looks like this</td><td></td>
	</tr>
	<tr>
		<td><img src="assets/input_210_epochs.jpg" alt="input_210_epochs" width="200"/></td>
		<td><img src="assets/generated_after_210_epochs.jpg" alt="generated_after_210_epochs" width="200"/></td> 
	</tr>
</table>

Examine the parts of the generator loss over time, the following values are observable:<br />

<table>
	<tr>
		<td><img src="assets/g_content_loss.png" alt="g_content_loss" width="400"></td>
		<td><img src="assets/g_adversarial_loss.png" alt="g_adversarial_loss" width="400">
	</tr>
	<tr>
	</tr>
</table>

<p>So the content loss is magnitudes higher than the adversarial loss.<br />
Maybe my calculation of the content loss is wrong? Should it be much lower? As the generated images preserve the content very good, I concentrate on the adversarial loss.<br />
Maybe I use the wrong VGG-Model? Image preservation is not the problem, therefore I concentrate on comic style.<br />
As the adversarial loss is responsible for the comic-effect, I try a much lower $\omega$, to balance the values of <code>g_content_loss</code> and <code>g_adversarial_loss</code> on an equal level for the next training round.</p>

<h2><a name="tc5_7">Training round 2</a></h2>
<p>As <code>g_content_loss</code> has values of $4e+5$, I choose $\omega=0.00001$. After 210 epochs, the result is the following:</p>
<table>
<tr>
<td>
<img src="assets/round_2/g_content_loss.png" alt="g_content_loss" width="400">
</td><td>
<img src="assets/round_2/g_adversarial_loss.png" alt="g_adversarial_loss" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_2/1585049184_input.jpg" alt="example_input_round_2_1" width="400">
</td><td>
<img src="assets/round_2/1585049184.jpg" alt="example_result_round_2_1" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_2/1585049204_input.jpg" alt="example_input_round_2_2" width="400">
</td><td>
<img src="assets/round_2/1585049204.jpg" alt="example_result_round_2_2" width="400">
</td>
</tr>
</table>

<p>The optimization of the content loss falls a little behind in comparison to training round 1, where $\omega$ was much higher and therefore plays a bigger role in the total loss.<br />
On the other hand, the optimization of the adversarial loss gets better, due it plays a bigger role for the total loss.<br />
As seen in the example images, the comic effect starts to kick in and the content is still preserved.</p>

<p>As a next trial, I set $\omega=0$, to have the optimization effect on the adversarial loss only.</p>

<h2><a name="tc5_8">Training round 3</a></h2>
<p>With $\omega=0$, the trained model looses the content information but the comic-style hits through:</p>

<table>
<tr>
<td>not part of the loss result due to set to zero, but plotted here to visualize!
<img src="assets/round_3/g_adversarial_loss.png" alt="g_content_loss" width="400">
</td><td>
<img src="assets/round_3/g_content_loss.png" alt="g_content_loss" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_3/example_input_round_3_1.jpg" alt="example_input_round_3_1" width="400">
</td><td>
<img src="assets/round_3/example_result_round_3_1.jpg" alt="example_result_round_3_1" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_3/example_input_round_3_2.jpg" alt="example_input_round_3_2" width="400">
</td><td>
<img src="assets/round_3/example_result_round_3_2.jpg" alt="example_result_round_3_2" width="400">
</td>
</tr>
</table>

<p>As comic-style starts to hit through around $\omega=0.00001$ and has full dominance at $\omega=0$, I try some $\omega$ between these values for the next round.</p>

<h2><a name="tc5_9">Training round 4</a></h2>
<p>Set $\omega = 0.00001 / 2 = 0.000005$</p>
<p>Unfortunately, I did not clean up my tensorboard results folder before re-training, so the results are plotted to the existing ones from round 3.</p>

<table>
<tr>
<td>
<img src="assets/round_4/g_content_loss.png" alt="g_content_loss" width="400">
</td><td>
<img src="assets/round_4/g_adversarial_loss.png" alt="g_content_loss" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_4/example_input_round_4_1.jpg" alt="example_input_round_4_1" width="400">
</td><td>
<img src="assets/round_4/example_result_round_4_1.jpg" alt="example_result_round_4_1" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_4/example_input_round_4_2.jpg" alt="example_input_round_4_2" width="400">
</td><td>
<img src="assets/round_4/example_result_round_4_2.jpg" alt="example_result_round_4_2" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_4/example_input_round_4_3.jpg" alt="example_input_round_4_3" width="400">
</td><td>
<img src="assets/round_4/example_result_round_4_4.jpg" alt="example_result_round_4_4" width="400">
</td>
</tr>
<tr>
<td>
<img src="assets/round_4/example_input_round_4_5.jpg" alt="example_input_round_4_5" width="400">
</td><td>
<img src="assets/round_4/example_result_round_4_5.jpg" alt="example_result_round_4_5" width="400">
</td>
</tr>
</table>
<p>Not every image is transformed as desired, but the results are there :-)</p>

<h2><a name="tc6">Optimizer</a></h2>
<p>In the paper, the used optimizer is not mentioned, I decide to choose adam.</p>

<p>For hyperparameter-tuning, I decided to go with the same parameters mentioned in the DCGAN-paper <a href="#ref_13">[13]</a>.</p>

<h2><a name="tc7">Transforming an image from local filesystem</a></h2>
For transforming an image from local filesystem, pre-trained weights are provided <a href="https://github.com/TobiasSunderdiek/cartoon-gan/releases"> within the release here.</a>
<ul>
	<li>download and unzip <code>generator_release.pth.zip</code> to root path of this project</li>
	<li>install necessary libs via <code>make install-transform</code></li>
	<li>transform image (*.jpg, *.png) from local filesystem via <code>make transform IMAGE=path_to_image</code></li>
		<ul>
			<li>e.g.: <code>make transform IMAGE=~/Pictures/photo.jpg</code></li>
		</ul>
	<li><code>transformed.JPG</code> is created</li>
</ul>

<h2><a name="tc8">Credits</a></h2>
Thanks to the authors Chen et al.[1] of the paper for their great work.

<h2><a name="notes">Notes/next steps</a></h2>
	<ul>
		<li>alternative lib for image processing: https://github.com/albu/albumentations</li>
		<li>figure out which variant of VGG to use (VGG-16?), and if the pre-training in the referenced paper is the same as the pre-trained pytorch version</li>
		<li>do I use correct normalization-method in content loss</li>
		<li>in which order is the discriminator trained regarding photos, cartoons with smoothed edges and then genereated images?</li>
		<li>evaluate result with existing model http://cg.cs.tsinghua.edu.cn/people/~Yongjin/CartoonGAN-Models.rar?</li>
		<li>did I split the loss function correctly for the D and G model, and content loss only for G?</li>
		<li>plot results directly from vars via method</li>
		<li>in the paper 6.000 photo images and 2.000 - 4.000 cartoon images are used for training, how is this done with unbalanced datasets?</li>
		<li>is batch_size of 16 correct? Tried 32 before, but got CUDA OOM</li>
		<li>for image downloader: catch exception if image is truncated/check if zipping adds additional folder within .zip in create_smoothed_images.py</li>
	</ul>
<h2><a name="refs">References</a></h2>

<a name="ref_1">[1]</a> http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf<br />
<a name="ref_2">[2]</a> https://www.kaggle.com/alamson/safebooru<br />
<a name="ref_3">[3]</a> http://cocodataset.org<br />
<a name="ref_4">[4]</a> https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb<br />
<a name="ref_5">[5]</a> http://images.cocodataset.org/annotations/annotations_trainval2017.zip<br />
<a name="ref_6">[6]</a> https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0<br />
<a name="ref_7">[7]</a> https://towardsdatascience.com/is-the-transposed-convolution-layer-and-convolution-layer-the-same-thing-8655b751c3a1<br />
<a name="ref_8">[8]</a> https://developers.google.com/machine-learning/gan/generator<br />
<a name="ref_9">[9]</a>  https://towardsdatascience.com/only-numpy-implementing-gan-general-adversarial-networks-and-adam-optimizer-using-numpy-with-2a7e4e032021<br />
<a name="ref_10">[10]</a> https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a<br />
<a name="ref_11">[11]</a>  https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c<br />
<a name="ref_12">[12]</a> https://machinelearningmastery.com/generative-adversarial-network-loss-functions/<br />
<a name="ref_13">[13]</a> https://arxiv.org/pdf/1511.06434.pdf<br />
</div>
</div>
</body>
</html>
